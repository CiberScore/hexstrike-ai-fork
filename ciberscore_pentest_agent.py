import asyncio
import json
import textwrap
import requests
from fastmcp import Client
import logging
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"  # change if using different LLM
MCP_SERVER = "/home/csai/autopentest_module/hexstrike-ai-fork/hexstrike_mcp.py"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def call_llm(prompt: str, model: str = "deepseek-r1:32b", format_json: bool = True):
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }
    if format_json:
        payload["format"] = "json"
    resp = requests.post(OLLAMA_URL, json=payload, timeout=1200)
    resp.raise_for_status()
    data = resp.json()
    if "response" in data and not format_json:
        return data["response"]
    return data.get("response") or data

SYSTEM_INSTRUCTION = textwrap.dedent("""
You are an autonomous pentest agent. You have the following tools available (name, arguments and brief):
{tools}

Respond ONLY in JSON with exactly one of:
1) {{ "action": "call_tool", "tool": "<tool_name>", "args": ~{{ ... }} }}
2) {{ "action": "finish", "report": "<final textual report of findings>" }}

If calling a tool, ensure args match the tool schema (types: string, int...). 
Do not attempt exploits unless the user explicitly allowed exploitation. Confirm the target is authorized before anything destructive.

Remember: Your replies MUST be valid JSON (no extra prose). 
""").strip()

async def build_system_prompt(client):
    tools = await client.list_tools()
    tools_desc = []
    for t in tools:
        name = t.name
        doc = t.description
        input_schema = t.inputSchema
        output_schema = t.outputSchema
        tools_desc.append(f"- {name}: {doc} ; input_schema: {json.dumps(input_schema)} ; output_schema: {json.dumps(output_schema)}")
    return SYSTEM_INSTRUCTION.format(tools="".join(tools_desc))

async def run_agent(initial_instruction: str, model="llama3.2"):
    async with Client(MCP_SERVER) as client:
        await client.ping()

        system_prompt = await build_system_prompt(client)
        conversation = [{"role": "user", "content": initial_instruction}]
        max_iters = 25
        for iteration in range(max_iters): # This must be modified on future versions
            prompt_messages = system_prompt + "\n\n"
            for msg in conversation:
                prompt_messages += f"{msg['role'].upper()}: {msg['content']}\n\n"

            llm_out = call_llm(prompt_messages, model=model, format_json=True)
            if isinstance(llm_out, (str,)):
                try:
                    action = json.loads(llm_out)
                except Exception as e:
                    conversation.append({"role":"assistant", "content": f"LLM returned invalid JSON: {str(e)}; raw:{llm_out}"})
                    continue
            else:
                action = llm_out

            if not isinstance(action, dict) or "action" not in action:
                conversation.append({"role":"assistant", "content": f"LLM returned unexpected structure: {json.dumps(action)}"})
                continue

            if action["action"] == "finish":
                report = action.get("report", "")
                logger.info("=== FINAL REPORT ===")
                logger.info(report)
                return {"status":"finished","report":report}

            if action["action"] == "call_tool":
                tool = action.get("tool")
                args = action.get("args", {})
                if not tool:
                    conversation.append({"role":"assistant", "content": "Invalid tool call: missing tool name."})
                    continue

                try:
                    logger.info(f"[agent] calling tool {tool} with args {args}")
                    result = await client.call_tool(tool, args)
                    try:
                        res_text = json.dumps(result.data) if hasattr(result, "data") else str(result)
                    except Exception:
                        res_text = str(result)
                    conversation.append({"role":"tool", "content": f"{tool} => {res_text}"})
                except Exception as e:
                    conversation.append({"role":"tool", "content": f"{tool} ERROR => {str(e)}"})

        return {"status":"gave_up","reason":"max_iters", "conversation":f"{conversation}"}
def start_pentest(target: str):
    standard_prompt = f"Perform a pentest on {target} -- you have explicit permission to test and to enumerate services; do NOT attempt to exploit unless I say so."
    result = asyncio.run(run_agent(standard_prompt))
    logger.info(result)
    return result
